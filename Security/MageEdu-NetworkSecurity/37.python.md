# Python

## 字符串

**检查字符串**

```
>>> txt = "hello world"
>>> "edu" in txt
False
>>> "edu" not in txt
True
```

**字符串拼接**

```
>>> "hello" + " world"
'hello world'
```

**字符串格式化**

```
>>> age = 100
>>> txt = "I am {} years old."
>>> txt.format(age)
'I am 100 years old.'
```

**转义**

```
>>> txt = 'I\'m Tom'
>>> txt
"I'm Tom"
>>> txt = r'I\'m Tom'  #不进行转义
>>> txt
"I\\'m Tom"
```

**计算长度**

```
len(txt)
```

## 列表

索引、切片、合并

**切片**

步进

```
>>> a = "python"
>>> a[::-1]
'nohtyp'
>>> a[::2]
'pto'
```

**合并**

```
"".join(fruits)[::-1]  # 合并列表并反转
```

**append**

追加元素

**赋值**

```
>>> le = ['a','b','c','d','e','f','g']
>>> le[2:5]
['c', 'd', 'e']
>>> le[2:5] = []
>>> le
['a', 'b', 'f', 'g']
```

**斐波那契**

```
>>> a,b = 0,1
>>> while a < 10:
...     a, b = b, a + b
...     print(b)
...
1
2
3
5
8
13
21
```

**print**

```
>>> txt = "hello"
>>> print(txt, end="!")
hello!>>>

>>> print("hello", "world", sep=" | ")
hello | world

>>> name = "Tom"
>>> age = 10
>>> print("%s is %s." %(name,age))
Tom is 10.
```


## 流程控制

**if**

```
if age < 10:
    print(1)
elif age < 20:
    print(2)
else:
    print(3)
```

**for**

```
for i in range(10):
    print(i)
```

遍历字典

```
>>> users = {"tom":"active", "jack":"inactive", "jerry":"active"}
>>> for k,v in users.items():
...     print(k,v)
...
tom active
jack inactive
jerry active
>>> for k,v in users.copy().items():
...     if v == "inactive":
...             del(users[k])
```

**break, continue, pass**


**match**

```
def http_error(status):
    match status:
        case 400:
            return "Bad request"
        case 404:
            return "Not found"
        case 401 | 402 | 403:
            return "Not allowed"
        case 500:
            return "Internal server error"
        case _:
            return "unknown error"
```


## 函数


**默认值**

```
def test(a, b, c=1):
  pass
```

**关键字参数**

```
test(a=1,b=2)
```

****args**

```
>>> def test(*args):
...     for arg in args:
...             print(arg)
...
>>> test("hello", "world")
hello
world
```

**仅位置参数**

使用 `/` 标记

```
def test(pos_only, /, standard, *, kw_only):
    pass
```

## 列表推导

```
[x**2 for x in range(10)]

# 等效
list(map(lambda x: x**2, range(10)))
```


## 文件操作


```
with open("mage.txt","a+") as f:
    f.write("1111、n")

f = open("maage.txt","r")
f.readline()
f.close()

for index, value in enumerate(f):
    print(index, value)
```

## Json

```
import json

d = {
  "name": "Tom",
  "age": 10
}

j = json.dumps(d)
json.loads(j)
```

## 类

**call 方法**

调用对象时执行

```
class Test:
    def __call__():
        print("calling")
t = Test()
t()
```

**classmethod**

不用实例化即可调用

**staticmethod**

**property**

按照调用属性的方式调用方法

```
class Test:
    def __init__(self, age):
        self.__age = age
    @property
    def age():
        return self.__age
```

**私有变量**


## 迭代器

```
__iter__

__next__
```

## 生成器

```
yield

next()
```

## 闭包

在函数里面由定义了一个内函数，内函数引用了外函数的临时变量，外函数返回内函数的引用

外函数执行结束后，内部变量不会释放掉

```
def outer(a):
    b = 10
    def inner():
      print(a+b)
    return inner
```


## *args 和 **kwargs

解包

```
>>> def add(*args):
...     return sum(args)
...
>>> add(1,2,3)
6
```

## 装饰器

为现有代码添加额外功能

装饰器就是一个闭包，把函数当作一个参数返回一个代替版函数

```

```

## 匿名函数

```
s = lambda a,b: a+b
```

## 异常

```
try:

except:

finally:
```

抛出异常

```
raise 异常实例，异常类
```

**异常链**

```
raise RuntimeError from exec
```


## 虚拟环境

```
python3 -m venv tutoral-env
```



## 正则

re

## requests 模块


## 数据库

```
pip install mysql-connector
```


**orm**

slqalchemy


## 多进程、多线程


### Thread 类

```python
import threading
import time

def worker():
    #print("I am working....")
    #print("finished")
    while True:
        time.sleep(0.5)
        print("I am working")

def add(x, y):
    print('{} + {} = {}'.format(x, y, x+y, threading.current_thread().ident))

t = threading.Thread(target=worker, name="worker")
t.start() #启动线程

t2 = threading.Thread(target=add, name="add", args="")

```


### concurrent.futures 包


## 爬虫

1. python + selenium
2. scrapy




CTF
AWD


###scrapy

```python
import scrapy

class MageduSpider(scrapy.Spider):
    name = "Magedu"

    def start_request(self):
        urls = [
        'https://quotes.toscrape.com/page/1/',
        'https://quotes.toscrape.com/page/2/'
        ]

        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

      def parse(self, response, **kwargs):
          page = response.url.split("/")[-2]
          filename = f'quotes-{page}.html'
          with open(filename, 'wb') as f:
              f.write(response.body)
          self.log(f'saved file {filename}')
```


**eg2**

```python
import scrapy

class MageduSpider(scrapy.Spider):
    name = "Magedu"

    start_urls = [
        'https://quotes.toscrape.com/page/1/',
        'https://quotes.toscrape.com/page/2/'
    ]


      def parse(self, response, **kwargs):
          page = response.url.split("/")[-2]
          filename = f'quotes-{page}.html'
          with open(filename, 'wb') as f:
              f.write(response.body)
          self.log(f'saved file {filename}')

```


**交互式 shell**

```
$ scrapy shell url
>>> response.css('title')
>>> response.css('title::text')
>>> response.css('title::text').getall()
>>> response.css('title::text').re(r'Quotes.*')
>>> response.xpath('//title/text()').getall()
>>> quote = response.css("div.quote")[0]
>>> text = quote.css("span.text:text").get()
>>> tags = quote.css("div.tags a.tag::text").getall()
>>> quote.css("div.tags a.tag::attr(href)").getall()
```


**自动爬取**

```python
import scrapy

class MageduSpider(scrapy.Spider):
    name = "Magedu"

    start_urls = [
        'https://quotes.toscrape.com/page/1/'
    ]


      def parse(self, response, **kwargs):
          for quote in response.css("div.quote"):
              yield {
                  'text': quote.css("span.text::text").get(),
                  'author': quote.css("small.author::text").get()
              }

          next_page = response.css("li.next a::attr(href)").get()
          if next_page is no None:
              next_page = response.urljoin(next_page)
              scrapy.Request(url=next_page, callback=self.parse)

          # 另一种方法
          for a in response.css("ul.pager a"):
              yield response.follow(a, callback=self.parse)

          # 更简洁的一种方法
          yield from response.follow_all(css="ul.pager a", callback=self.parse)
```


**参数处理**

```python
def start_requests(self):
    url = "https://quotes.toscrape.com/"
    tag = getattr(self, 'tag', None)
    if tag is not None :

```

**Item 使用**

继承 scrapy Item 类

**pipleline**

1. open_spider
2. close_spider
3. process_item

**中间件**


## 异步IO


async await
