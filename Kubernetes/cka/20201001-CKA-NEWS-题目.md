1. 考察RBAC，要求创建指定namespace下的serviceAccount，并创建Role/ClusterRole，绑定serviceAccount。要求指定的sa对指定namespace有创建pod的权限（如果是对整个集群的话，需要用ClusterRoleBinding）

   

   ```
   example：
   
      namespace: ns01
   
      serviceAccount: ku001sa
   
     ClusterRole: ku001-ClusterRole
   
   ```

   ref: https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/

    创建sa：

   kubectl create sa ku001sa -n ns01

     创建ClusterRole。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ku001-ClusterRole
rules:
- apiGroups: [""]
  #
  # at the HTTP level, the name of the resource for accessing Secret
  # objects is "secrets"
  resources: ["pods"]
  verbs: ["create"]
```

绑定RoleBinding

```yaml
apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows "jane" to read pods in the "default" namespace.
# You need to already have a Role named "pod-reader" in that namespace.
kind: RoleBinding
metadata:
  name: read-pods
  namespace: ns01
subjects:
- kind: ServiceAccount
  name: ku001sa
  namespace: ns01
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ku001-ClusterRole
```



2. 创建ingress名为ku002-ingress，将service 2379端口暴露出来，并通过curl -kL <clusterIp>/hi来验证，如果返回hi，则证明有效。

ref：https://kubernetes.io/zh/docs/concepts/services-networking/ingress/



```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ku002-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /hi
        pathType: Prefix
        backend:
          service:
            name: hi
            port:
              number: 2379
```



3. 升级集群里的主节点上的kubelet和kubeadm，版本为1.19.0。同时将control panel里的组件也升级。注意升级之前需要将主节点置为不可调度，然后升级完成后将主节点uncordon。

一定要在主节点上操作。默认进入集群是node节点。

```
ssh master-node

sudo -i #切换root账户
kubectl drain node master-node -ignore-daemonsets
apt-cache policy kubelet | grep 1.19 # 获取系统里存在的版本
apt-get upgrade -y kubelet-1.19.0-00 
apt-get upgrade -y kubeadm-1.19.0-00
#执行kubectl version 查看client信息。
找到static pod路径，默认为/etc/kubernetes/manifests
修改kube-apiserver.yaml,kube-controllermanager.yaml,kube-scheduler.yaml,将image的版本改为1.19.0
再次检查
发现server的版本也变为1.19.0了

最后，kubectl uncordon masternode
```

4. 跟第三题同样的集群，备份集群。并用它提供的备份文件恢复。

No configuration context change required for this item Create a snapshot of the etcd instance running at https://127.0.0.1:2379 saving the snapshot to the file path /data/backup/etcd-snapshot.db The etcd instance is running etcd version 3.2.18 The following TLS certificates/key are supplied for connecting to the server with etcdctl CA certificate: /opt/KUCM0612/ca.crt Client certificate: /opt/KUCM00612/etcdclient.crt Client key: /opt/KUCM00612/etcd-client.key，然后用/tmp/historyetcd-backup.db恢复集群。
解析：备份etcd，手册中有该命令，需要指定证书的话，看下etcdctl --help
参看地址：https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
需要指定证书，所以你使用etcdctl --help查看下证书相关的字段

```
export ETCDCTL_API=3
etcdctl snapshot save --help
etcdctl --endpoints= https://127.0.0.1:2379 --cert="/opt/KUCM000613/etcd-client.crt" --
cacert="/opt/KUCM00612/ca.crt" --key="/opt/KUCM00612/etcd-client.key" snapshot save
/data/backup/etcd-snapshot.db
```

接着既可以检查下你备份的文件：

```
ETCDCTL_API=3 etcdctl --write-out=table snapshot status /data/backup/etcd-snapshot.db
有以下输出，就没问题
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fe01cf57 |       10 |          7 | 2.1 MB     |
+----------+----------+------------+------------+

恢复：
etcdctl snapshot restore /tmp/historyetcd-backup.db
```



5. 创建networkPolicy，针对namespace ku004-ns下的pod，只允许同样namespace下的pod访问，并且可访问pod的80端口。

不允许不是来自这个namespace的pod访问。

不允许不是监听80端口的pod访问。

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace:  ku004-ns
spec:
  podSelector: {}

  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          project: ku004-ns
    ports:
    - protocol: TCP
      port: 80
```

1. 创建daemonSet ku006，image=nginx，不要影响现有含有Taints的node。



```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ku006
  labels:
    k8s-app: ku006
spec:
  selector:
    matchLabels:
      name: ku006
  template:
    metadata:
      labels:
        name: ku006
      containers:
      - name: nginx
        image: nginx
 
```



7. 查看集群里含有app=front-service的pod, 写入文件/opt/ku007/pod.txt

```
kubectl get svc --show-labels #获取svc的label
kubectl get pod -A -l app=front-service > /opt/ku007/pod.txt
```



8. 查看集群里指定namespace下消耗CPU资源最多的pod，把名字写入文件/opt/ku008/ku008.txt

``` 
kubectl top pod -n <namespace> | sort -g -k2 > /opt/ku008/ku008.txt
```

9. 创建一个pod ku009，其中包含1到4个container。nginx+redis+jenkins

```
apiVersion: v1
kind: Pod
metadata:
  name: ku009
spec:
  containers:
    - name: nginx
      image: nginx
    - name: redis
      image: redis
    - name: jenkins
      image: jenkins
```

10. 查看指定pod中log信息，把Error信息存入文件

```
kubectl logs <podName> | grep -i Error > /tmp/ku10.txt
cat /tmp/ku10.txt
```

11. 将deployment ku11，采用nodePort的方式暴露出来，端口为80。

```
kubectl expose deploy ku11 --type="NodePort" --port=80 --target-port=80 --name=ku11-service
kubectl get ep -o wide #查看endpoints是否生成
```

12. 创建指定namespace ku12-ns下的jenkins pod。

````
kubectl create ns ku12-ns

apiVersion: v1
kind: Pod
metadata:
  name: ku12
  namespace: ku12-ns
spec:
  containers:
    - name: jenkins
      image: jenkins
````

13. 集群中有一个节点node无法连接到集群里，请帮忙恢复集群，并保证修改是永久生效的。可以通过ssh node-ip登陆到节点上去，获取权限可以通过sudo -i.

```
kubectl get node
ssh node-ip
sudo -i
检查kubelet是否正常
systemctl status kubelet  发现kubelet没有启动
systemctl start kubelet
systemctl enable kubelet 保证永久生效
exit
返回到master节点，执行kubectl get node	发现都是ready。
```

14. 修改deployment的image版本，记录，并回滚

``` -
题目直接说的是pod，然后发现集群里有两个pod，猜测应该有deploy，用命令查看了下， 果然。那么题目就很简单了

kubectl get deploy <deploymentName> -o yaml

kubectl set image deploy <deploymentName> nginx=nginx:1.16.1 --record
kubectl get deploy <deploymentName> -o yaml | grep image
kubectl rollout status deploy <deploymentName>
回滚
kubectl rollout undo deployment <deploymentName>
再次检查下
kubectl get deploy <deploymentName> -o yaml | grep image

```



14. 根据storageClass创建pvc, 读取权限为ReadWriteOnce，并通过patch或者edit的方式，将pvc的存储改为5Gi，并记录

example：storageClass：ku15-sc

​					pvc: ku15-pvc

​                    storage：3Gi

​                   pod：ku15-pod

​                  mountPath: /var/data

```
storageClass是一种直接定义pvc，集群会根据pvc创建对应的pv

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ku15-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 3Gi
  storageClassName: ku15-sc
  
  通过kubectl apply -f pvc.yaml
  
  创建pod关联pvc。
apiVersion: v1
kind: Pod
metadata:
  name: ku15-pod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/data"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: ku15-pvc

  
  kubectl get pvc # 检查下是否正常绑定
  kubectl get pod | grep ku15-pod
  
  在通过patch来将storage改为5Gi。获取json路径可copy pvc中antonation来快速编写。毕竟手打还是很慢的。
  kubectl patch pvc ku15-pvc -p '{"resources":{"requests":{"storage":"5Gi"}}}' --record
```



14. sideCar log。告诉一个运行的pod，要求加入一个sidecar container，共享pod的log。

https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/

这个还是挺难的。首先根据pod，获取yaml。系统里没有原始的文件，所以我是通过-o yaml的方式写入文件。（运行时态的pod，是不允许加入新的container），然后共享目录，那么就用到了emptyDir

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
```

14. 查看集群里不包含Taint的node数

```
kubectl describe node | grep -i taints | wc -l
kubectl get node 	
然后两数相减
```



### 下面列一些可能会考到，但是这几份都没有的。



1、Set configuration context $kubectl config use-context k8s Perform the following tasks: Add an init container to lumpy-koala(which has been defined in spec file /opt/kucc00100/pod-specKUCC00928.yaml). The init container should create an empty file named /workdir/calm.txt. If /workdir/calm.txt is not detected, the Pod should exit. Once the spec file has been updated with the init container definition, the Pod should be created
解析：
这道题在/opt/kucc00100/pod-specKUCC00928.yaml路径下已经有写好的Yaml了，但是还未在集群中创建该对象。所以你上去最好先kubectl get po | grep pod名字。发现集群还没有该pod。所以你就先改下这个Yaml,然后apply.先创建Initcontainer,然后在里面创建文件，/workdir目录明显是个挂载进度的目录，题目没规定，你就定义empDir类型。这边还要用到liveness检查。
参考网址：
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#using-init-containers
https://kubernetes.io/docs/concepts/storage/volumes/#emptydir
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

```
apiVersion: v1
kind: Pod
metadata:
  name: pod-specKUCC00928
spec:
  containers:
  - image: nginx
    name: test-container
    livenessProbe:
      exec:
        command:
        - cat
        - /workdir/calm.txt
    volumeMounts:
    - mountPath: /workdir
      name: cache-volume
  initContainers:
  - name: init-container
    image: busybox:1.28
    command: ['sh', '-c', "touch /workdir/calm.txt"]
    volumeMounts:
    - mountPath: /workdir
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
```

2、Set configuration context $kubectl config use-context k8s Create a Kubernetes Secret as follows: Name: super-secret credential: blob, Create a Pod named pod-secrets-via-file using the redis image which mounts a secret named super-secret at /secrets. Create a second Pod named pod-secretsvia-env using the redis image, which exports credential as CREDENTIAL
解析：创建secret，并在pod中通过Volume和环境变量使用该secret
参看地址：https://kubernetes.io/zh/docs/concepts/configuration/secret/

```
对blob进行base64编码，然后才能放入secret
echo -n 'blob' | base64
```

创建yaml：

```
apiVersion: v1
kind: Secret
metadata:
  name: super-secret
type: Opaque
data:
  credential: YWRtaW4=
```

kubectl apply -f secret.yaml
创建pod:pod-secrets-via-file

```
apiVersion: v1
kind: Pod
metadata:
  name: pod-secrets-via-file
spec:
  containers:
  - name: pod-secrets-via-file
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/secrets"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: super-secret
```

kubectl apply -f pod-secrets-via-file.yaml
创建pod:pod-secretsvia-env

```
apiVersion: v1
kind: Pod
metadata:
  name: pod-secretsvia-env
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: supersecret
            key: username
      - name: CREDENTIAL
        valueFrom:
          secretKeyRef:
            name: supersecret
            key: credential
  restartPolicy: Never
```

kubectl apply -f pod-secretsvia-env.yaml
检验：
kubectl exec -ti pod-secretsvia-env -- env ##会打印出很多环境变量，看下你定义的在不在，最后进pod: echo $CREDENTIAL 确认下结果
kubectl exec -ti pod-secrets-via-file -- ls /secrets



3、Set configuration context $kubectl config use-context k8s Create a deployment as follows: Name: nginxdns Exposed via a service : nginx-dns Ensure that the service & pod are accessible via their respective DNS records The container(s) within any Pod(s) running as a part of this deployment should use the nginx image. Next, use the utility nslookup to look up the DNS records of the service & pod and write the output to /opt/service.dns and /opt/pod.dns respectively. Ensure you use the busybox:1.28 image (or earlier) for any testing, an the latest release has an upstream bug which impacts the use of nslookup
解析：创建service和deployment，然后解析service的dns和pod的dns，并把解析记录保存到指定文件
我不喜欢用命令创建deployment和Service，全用yaml文件，用起来比较明确。
去这里复制yaml:https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/
deployment.yaml:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxdns
spec:
  selector:
    matchLabels:
      app: nginxdns
  replicas: 1
  template:
    metadata:
      labels:
        app: nginxdns
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - name: http
              containerPort: 80
```

service.yaml:

```
apiVersion: v1
kind: Service
metadata:
  name: ngixdns
spec:
  selector:
    app: nginxdns
  ports:
  - protocol: TCP
    port: 80
    targetPort: http
```

去这里复制busybox的yaml:
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
busybox.yaml:

```
apiVersion: v1
kind: Pod
metadata:
  name: busybox-test
  labels:
    app: busybox-test
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600'] #sleep 3600一定要啊，不然busybox运行后直接退出
```

kubectl apply -f busybox.yaml
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
kubectl exec -ti busybox-test -- nslookup nginx-dns > /opt/service.dns
kubectl exec -ti busybox-test -- nslookup podup > /opt/pod.dns


